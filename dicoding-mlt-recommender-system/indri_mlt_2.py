# -*- coding: utf-8 -*-
"""indri_mlt2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MyjehyGPqStFwPlAOzwVEnWVXd3w-QXG

# Machine Learning Terapan - Submission Sistem Rekomendasi

Nama : Indri Windriasari <br>
Email: indriwindriasari2511@gmail.com

## 1 Library Import
"""

# Library untuk manipulasi data dan array
import numpy as np
import pandas as pd

# Library untuk pemodelan dan evaluasi machine learning
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.metrics import RootMeanSquaredError

# Library untuk pengolahan teks dan kesamaan vektor
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Library untuk visualisasi data
import seaborn as sns
import matplotlib.pyplot as plt

"""## 2 Data Understanding

### 2.1 Data Loading

Tahapan ini memuat dataset [Book Recommendation Dataset](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset) dari Kaggle Dataset dan menampilkan informasi dari dataset.
"""

# Membuat direktori baru bernama kaggle
!rm -rf ~/.kaggle && mkdir ~/.kaggle/

# Menyalin berkas kaggle.json pada direktori aktif saat ini ke direktori kaggle
!mv kaggle.json ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json

# Download dataset
!kaggle datasets download -d arashnic/book-recommendation-dataset

# Ekstrak berkas zip
!unzip /content/book-recommendation-dataset.zip

"""Ekstraksi berkas dilakukan dengan menggunakan perintah `!unzip`, sehingga diperoleh tiga file csv, yaitu `Books.csv`, `Ratings.csv`, dan `Users.csv`."""

books_df = pd.read_csv('Books.csv')
ratings_df = pd.read_csv('Ratings.csv')
users_df = pd.read_csv('Users.csv')

users_df.head()

books_df.head()

ratings_df.head()

"""Dengan Library Pandas, ketiga file csv dapat dibaca dalam bentuk dataframe.

### 2.2 Kondisi Dataset & Exploratory Data Analisis (EDA)

Ukuran atau dimensi pada setiap dataset
"""

print(f'Dimensi dataset users     : {users_df.shape}')
print(f'Dimensi dataset buku     : {books_df.shape}')
print(f'Dimensi dataset rating    : {ratings_df.shape}')

"""Exploratory Data Analysis (EDA) sebagai inversitasi awal analisa data untuk masing-masing dataframe"""

def explore(df):
  """
    Fungsi ini digunakan untuk melakukan eksplorasi awal pada dataset yang diberikan dan memberikan informasi penting tentang setiap kolom dalam DataFrame, seperti:
      - Tipe data setiap kolom.
      - Persentase nilai yang hilang di setiap kolom.
      - Jumlah nilai unik yang ada di setiap kolom.
      - Daftar nilai unik dari setiap kolom.
  """
  with pd.option_context("display.max_colwidth", 20):
    info = pd.DataFrame()
    info['Tipe Data'] = df.dtypes
    info['Persentase Missing Value'] = df.isnull().sum()*100/len(df)
    info['Jumlah Nilai Unik'] = df.apply(lambda x: len(x.unique()))
    info['Nilai Unik'] = df.apply(lambda x: x.unique())
  return info.sort_values('Tipe Data')

"""#### 2.2.1 Dataset Users"""

explore(users_df)

users_df.describe()

"""Berdasarkan hasil eksplorasi terdapat missing value pada atribut `Age`. Maka, selanjutnya perlu dilakukan pemrosesan lebih lanjut pada tahap data preparation.

#### 2.2.2 Dataset Books
"""

explore(books_df)

"""Berdasarkaan hasil eksplorasi terdapat missing value pada atribut `Book-Author`, `Publisher`, dan `Image-URL-L`
. Maka, selanjutnya perlu dilakukan pemrosesan lebih lanjut pada tahap data preparation.

#### 2.2.1 Dataset Ratings
"""

explore(ratings_df)

ratings_df.describe()

# Menghitung jumlah masing-masing rating
rating_counts = ratings_df['Book-Rating'].value_counts().sort_index()

# Visualisasi jumlah setiap rating
plt.figure(figsize=(10, 6))
sns.barplot(x=rating_counts.index, y=rating_counts.values, palette='coolwarm')
plt.title('Distribusi Jumlah Rating')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.xticks(range(len(rating_counts.index)))

# Menambahkan angka di atas grafik
for i, value in enumerate(rating_counts.values):
    plt.text(i, value + 50, str(value), ha='center', va='bottom', fontsize=10, color='black')

plt.show()

"""Tidak terdapat missing value pada dataset Ratings.csv. Namun, berdasarkan hasil visualisasi, rating yang terbanyak pada dataset adalah rating 0. Hal ini menunjukkan bahwa sebagian besar data tidak memiliki rating eksplisit.

Data dengan rating 0 dapat menyebabkan bias dalam analisis, sehingga perlu dipertimbangkan untuk dihapus pada tahap data preparation. Hal ini dilakukan untuk memastikan hasil analisis dan rekomendasi menjadi lebih akurat.

## 3 Data Preparation

### 3.1 Penggabungan Data

Penggabunan data ISBN dan data User untuk melihat jumlah data secara keseluruhan
"""

isbn_concat = np.concatenate((
    books_df.ISBN.unique(),
    ratings_df.ISBN.unique()
))

isbn_concat = np.sort(np.unique(isbn_concat))
print(f'Jumlah Data Buku berdasarkan ISBN : {len(isbn_concat)}')

ratings_df.rename(columns={
                  'User-ID'     : 'user_id'},
                  inplace=True)

users_df.rename(columns={
                  'User-ID'     : 'user_id'},
                  inplace=True)

user_concat = np.concatenate((
    ratings_df.user_id.unique(),
    users_df.user_id.unique()
))

user_concat = np.sort(np.unique(user_concat))
print(f'Jumlah User berdasarkan User-ID : {len(user_concat)}')

"""### 3.2. Mengatasi Missing Value

Pada tahap data understanding, `users_df` memiliki data missing pada atribut `Age` dengan persentase sebesar 34.75%. Missing value tersebut akan diisi dengan nilai modus atau nilai yang paling sering muncul dalam data Age.

Pendekatan ini dipilih karena nilai modus dianggap dapat mewakili pola umum dari data yang ada. Pengisian dilakukan menggunakan fungsi `.fillna()` bersama dengan fungsi `.mode()`, sehingga `Age` akan menjadi lengkap tanpa missing value, siap untuk dianalisis lebih lanjut.
"""

users_df.shape

users_df.isnull().sum()

users_df['Age'] = users_df['Age'].fillna(users_df['Age'].mode()[0])

users_df.isnull().sum()

# sanity check (harus sama seperti awal karena di fill oleh mode())
users_df.shape

"""Pada `books.df` atribut `Book-Author`, `Publisher`, dan `Image-URL-L` memiliki missing value."""

# cek ulang missing value
books_df.isnull().sum()

"""Untuk menangani hal ini, data dengan nilai kosong tersebut akan dihapus menggunakan fungsi .dropna(). Hal ini karena jumlahnya yang sangat kecil tidak akan memengaruhi analisis secara signifikan. Setelah proses ini dilakukan, pengecekan ulang menunjukkan bahwa tidak ada lagi data kosong (null) pada atribut tersebut, sehingga dataset siap untuk digunakan dalam tahap analisis selanjutnya."""

books_df = books_df.dropna()

books_df.isnull().sum()

# cek ulang missing value
ratings_df.isnull().sum()

"""Setelah dilakukan pengecekan terhadap dataset ratings, tidak ditemukan adanya missing value pada setiap atribut atau kolomnya. Namun, pada tahap Exploratory Data Analysis (EDA), visualisasi grafik menunjukkan bahwa sebagian besar data rating memiliki nilai 0, dengan jumlah sekitar 716.109 data."""

# Menghitung jumlah masing-masing rating
rating_counts = ratings_df['Book-Rating'].value_counts().sort_index()

# Visualisasi jumlah setiap rating
plt.figure(figsize=(12, 6))
sns.barplot(x=rating_counts.index, y=rating_counts.values, palette='coolwarm')
plt.title('Distribusi Jumlah Rating', fontsize=14)
plt.xlabel('Rating', fontsize=12)
plt.ylabel('Jumlah', fontsize=12)
plt.xticks(range(len(rating_counts.index)), fontsize=10)

# Menambahkan angka di atas grafik
for i, value in enumerate(rating_counts.values):
    plt.text(i, value + 5000, str(value), ha='center', va='bottom', fontsize=10, color='black')

plt.grid(axis='y', linestyle='--', linewidth=0.5, alpha=0.7)
plt.tight_layout()
plt.show()

"""Nilai rating 0 ini berpotensi menyebabkan bias pada analisis data, sehingga data dengan rating 0 akan dihapus dari dataset menggunakan fungsi filter. Data yang digunakan selanjutnya hanyalah data dengan nilai rating lebih dari 0, yaitu rating 1 hingga rating 10."""

# Memfilter dataset untuk rating 1 hingga 10
ratings_df = ratings_df[ratings_df['Book-Rating'] > 0]

# Menghitung jumlah masing-masing rating
rating_counts = ratings_df['Book-Rating'].value_counts().sort_index()

# Visualisasi jumlah setiap rating
plt.figure(figsize=(12, 6))
sns.barplot(x=rating_counts.index, y=rating_counts.values, palette='coolwarm')
plt.title('Distribusi Jumlah Rating (Tanpa Rating 0)', fontsize=14)
plt.xlabel('Rating', fontsize=12)
plt.ylabel('Jumlah', fontsize=12)
plt.xticks(range(len(rating_counts.index)), fontsize=10)

# Menambahkan angka di atas grafik
for i, value in enumerate(rating_counts.values):
    plt.text(i, value + 5000, str(value), ha='center', va='bottom', fontsize=10, color='black')

plt.grid(axis='y', linestyle='--', linewidth=0.5, alpha=0.7)
plt.tight_layout()
plt.show()

"""### 3.3 Mengatasi Data Duplikat"""

# Pengecekan duplikat pada masing-masing dataframe
duplicate_books = books_df.duplicated().sum()
duplicate_ratings = ratings_df.duplicated().sum()
duplicate_users = users_df.duplicated().sum()

# Menampilkan hasil
print(f"Jumlah data duplikat pada Books  : {duplicate_books}")
print(f"Jumlah data duplikat pada Ratings: {duplicate_ratings}")
print(f"Jumlah data duplikat pada Users  : {duplicate_users}")

"""Berdasarkan hasil pengecekan data duplikat pada masing-masing dataframe:

- Books: Tidak ditemukan data duplikat dalam tabel Books. Hal ini berarti setiap buku dalam dataset unik, baik dari segi ISBN maupun atribut lainnya.
- Ratings: Tidak terdapat data rating yang duplikat. Setiap kombinasi user dan buku memiliki rating yang unik atau tidak terduplikasi.
- Users: Tidak ditemukan data user yang duplikat. Setiap user dalam dataset memiliki ID yang unik dan informasi demografi yang tidak saling tumpang tindih.

### 3.4 Merge Data Rating dengan Data Buku

Merge data dilakukan dari hasil filter rating tanpa rating nol pada tahapan sebelumnya dengan data buku.

Sehingga dataframe `books_ratings_merge` akan berisi informasi gabungan dari kedua dataframe. Isinya akan mencakup data rating buku dari hasil filter`ratings_df` dan informasi buku dari `books_df`, di mana setiap baris menunjukkan rating untuk buku tertentu yang memiliki ISBN yang sama.
"""

books_ratings_merge = pd.merge(ratings_df, books_df, on=['ISBN'])
books_ratings_merge.head(5)

"""## 4 Modelling & Result

Berdasarkan pemahaman data sebelumnya, data buku, rating, dan pengguna cukup besar, mencapai ratusan hingga jutaan entri. Hal ini berpotensi meningkatkan biaya pemodelan, seperti waktu pemrosesan dan penggunaan resource. Oleh karena itu, untuk efisiensi, data yang digunakan akan dibatasi pada 10.000 untuk baris data buku dan 5.000 untuk baris data rating.
"""

books_df   = books_df[:10000]
ratings_df = ratings_df[:5000]

books_df.shape

ratings_df.shape

"""### 4.1 Content-based

#### 4.1.1 TF-IDF Vectorizer
"""

books_df.rename(columns={ 'Book-Author'     : 'book_author'},
                inplace=True)

# inisialisasi TfidfVectorizer
tfidf = TfidfVectorizer()

# melakukan perhitungan idf pada data book_author
tfidf.fit(books_df.book_author)

matrix_tfidf = tfidf.fit_transform(books_df.book_author)

# melihat ukuran matriks tfidf
matrix_tfidf.shape

# mengubah ke bentuk matriks
matrix_tfidf.todense()

books_df.rename(columns={ 'Book-Title'     : 'book_title'},
                inplace=True)

pd.DataFrame( matrix_tfidf.todense(),
              columns = tfidf.get_feature_names_out(),
              index   = books_df.book_title
).sample(20, axis=1).sample(5, axis=0)

"""#### 4.1.2 Cosine Similarity"""

cosine_sim = cosine_similarity(matrix_tfidf)
cosine_sim

cosine_sim_df = pd.DataFrame(
    cosine_sim,
    columns = books_df.book_title,
    index   = books_df.book_title
)

print(f'Cosine Similarity Shape : {cosine_sim_df.shape}')

# melihat similarity matriks
cosine_sim_df.sample(20, axis=1).sample(5, axis=0)

"""#### 4.1.3 Recommendation Result"""

def book_recommendations(book_title, similarity_data=cosine_sim_df, items=books_df[['book_title', 'book_author']], k=10):

    # mengambil data dengan menggunakan argpartition untuk melakukanpartisi secara tidak langsung sepanjang sumbu
    index = similarity_data.loc[:,book_title].to_numpy().argpartition(range(-1, -k, -1))

    # mengambil data dengan nilai similarity terbesar
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # menghapus judul buku  yang dicari agar tidak muncul pada daftar rekomendasi
    closest = closest.drop(book_title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

targeted_book = 'Devoted'
books_df[books_df.book_title.eq(targeted_book)]

# mendapatkan rekomendasi buku yang mirip
book_recommendations(targeted_book).drop_duplicates()

## 1. Content-Based Evaluation
def evaluate_content_based(books_df, cosine_sim_df, k=10, sample_size=278):
    """
    Evaluasi untuk content-based filtering
    """
    results = {
        'precision': [],
        'recall': [],
        'f1': [],
        'coverage': set()
    }

    # Batasi k agar tidak melebihi jumlah buku yang tersedia
    k = min(k, len(books_df) - 1)

    # Ambil sample untuk evaluasi
    sample_size = min(sample_size, len(books_df))
    sample_books = books_df.sample(n=sample_size, random_state=42)

    successful_evaluations = 0

    for _, book in sample_books.iterrows():
        try:
            # Dapatkan rekomendasi
            recommendations = book_recommendations(
                book['book_title'],
                similarity_data=cosine_sim_df,
                items=books_df[['book_title', 'book_author']],
                k=k
            )

            if len(recommendations) > 0:
                # Hitung metrics
                relevant_recs = len(recommendations[recommendations['book_author'] == book['book_author']])
                total_relevant = len(books_df[books_df['book_author'] == book['book_author']]) - 1

                precision = relevant_recs / k
                recall = relevant_recs / total_relevant if total_relevant > 0 else 0
                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

                results['precision'].append(precision)
                results['recall'].append(recall)
                results['f1'].append(f1)
                results['coverage'].update(recommendations['book_title'])

                successful_evaluations += 1

        except Exception as e:
            continue

    if successful_evaluations > 0:
        final_metrics = {
            'precision': np.mean(results['precision']),
            'recall': np.mean(results['recall']),
            'f1_score': np.mean(results['f1']),
            'coverage': len(results['coverage']) / len(books_df),
            'samples_evaluated': successful_evaluations
        }
    else:
        final_metrics = {
            'precision': 0,
            'recall': 0,
            'f1_score': 0,
            'coverage': 0,
            'samples_evaluated': 0
        }

    return final_metrics

# Evaluasi Content-Based
metrics_cb = evaluate_content_based(books_df, cosine_sim_df, k=5)

# Print detailed results
print("\nContent-Based Filtering Metrics:")
print("================================")
print(f"Precision@10 : {metrics_cb['precision']:.3f}")
print(f"Recall      : {metrics_cb['recall']:.3f}")
print(f"F1-Score    : {metrics_cb['f1_score']:.3f}")
print(f"Samples     : {metrics_cb['samples_evaluated']}")

"""### 4.2 Collaborative Filtering

#### 4.2.1 Penyandian (encoding) Fitur
"""

# mengubah User-ID  menjadi list tanpa nilai yang sama
user_ids = ratings_df.user_id.unique().tolist()

# melakukan encoding User-ID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}

# melakukan encoding angka ke User-ID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

print('List User-ID            : ', user_ids)
print('Encoded User-ID         : ', user_to_user_encoded)
print('Encoded angka ke User-ID: ', user_encoded_to_user)

# mengubah ISBN  menjadi list tanpa nilai yang sama
book_ids = ratings_df.ISBN.unique().tolist()

# melakukan encoding ISBN
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}

# melakukan encoding angka ke ISBN
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}

print('List ISBN               : ', book_ids)
print('Encoded ISBN            : ', book_to_book_encoded)
print('Encoded angka ke ISBN   : ', book_encoded_to_book)

"""Mapping User-ID dan ISBN ke dataframe"""

# mapping User-ID ke dataframe user
ratings_df['user'] = ratings_df.user_id.map(user_to_user_encoded)
# mapping ISBN ke dataframe buku
ratings_df['book'] = ratings_df.ISBN.map(book_to_book_encoded)

"""Mengecek Jumlah"""

num_users = len(user_encoded_to_user)
num_books = len(book_encoded_to_book)
print('Jumlah User: ', num_users)
print('Jumlah Buku: ', num_books)

ratings_df.rename(columns={ 'Book-Rating'     : 'book_rating'},
                inplace=True)

min_ratings = min(ratings_df.book_rating)
max_ratings = max(ratings_df.book_rating)
print('Minimal Rating: ', min_ratings)
print('Maksimal Rating: ', max_ratings)

# mengacak dataset
ratings_df = ratings_df.sample(frac=1, random_state=42)
ratings_df

"""#### 4.2.2 Data Splitting

Dataset dibagi dengan rasio 80:20. Hal ini berarti bahwa 80% dari data dgunakan untuk pelatihan model (training) dan 20% untuk uji.
"""

# membuat variabel yang mencocokan data user dengan data buku
x = ratings_df[['user', 'book']].values

# membuat variabel targer (y) untuk membuat rating dari hasil
y = ratings_df['book_rating'].apply(lambda x: (x-min_ratings) / (max_ratings-min_ratings)).values

# membagi dataset menjadi 80:20.
train_indices = int(0.8 * ratings_df.shape[0])

x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""#### 4.2.3 Training Model"""

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_books, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_books = num_books
        self.embedding_size = embedding_size

        self.user_embedding = layers.Embedding( # layer embedding user
            num_users,
            embedding_size,
            embeddings_initializer = 'he_normal',
            embeddings_regularizer = keras.regularizers.l2(1e-6))
        self.user_bias      = layers.Embedding(num_users, 1) # layer embedding user bias

        self.book_embedding = layers.Embedding( # layer embedding buku
            num_books,
            embedding_size,
            embeddings_initializer = 'he_normal',
            embeddings_regularizer = keras.regularizers.l2(1e-6))
        self.book_bias = layers.Embedding(num_books, 1) # layer embedding buku bias

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:,0])  # memanggil layer embedding 1
        user_bias   = self.user_bias(inputs[:, 0])      # memanggil layer embedding 2
        book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
        book_bias   = self.book_bias(inputs[:, 1])      # memanggil layer embedding 4

        dot_user_book = tf.tensordot(user_vector, book_vector, 2)
        x = dot_user_book + user_bias + book_bias

        return tf.nn.sigmoid(x)

model = RecommenderNet(num_users, num_books, 50) # inisialisasi model

model.compile(
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    loss      = tf.keras.losses.BinaryCrossentropy(),
    metrics   = [RootMeanSquaredError()]
)

# training model
history = model.fit(
    x               = x_train,
    y               = y_train,
    batch_size      = 8,
    epochs          = 100,
    validation_data = (x_val, y_val),
)

rmse     = history.history['root_mean_squared_error']
val_rmse = history.history['val_root_mean_squared_error']

plt.figure(figsize = (10, 4))

plt.subplot(1, 2, 1)
plt.plot(rmse,     label='RMSE')
plt.plot(val_rmse, label='Validation RMSE')
plt.title('Training and Validation Error')
plt.xlabel('Epoch')
plt.ylabel('Root Mean Squared Error')
plt.legend(loc='upper right')

loss     = history.history['loss']
val_loss = history.history['val_loss']
plt.subplot(1, 2, 2)
plt.plot(loss,     label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')

plt.show()

"""#### 4.2.4 Recommendation Result"""

# data_book   = books_df
# data_rating = ratings_df

datasetBook   = books_df
datasetRating = ratings_df

userId      = datasetRating.user_id.sample(1).iloc[0]
readedBooks = datasetRating[datasetRating.user_id == userId]

notReadedBooks = datasetBook[~datasetBook['ISBN'].isin(readedBooks.ISBN.values)]['ISBN']
notReadedBooks = list(
    set(notReadedBooks).intersection(set(book_to_book_encoded.keys()))
)

notReadedBooks = [[book_to_book_encoded.get(x)] for x in notReadedBooks]
userEncoder    = user_to_user_encoded.get(userId)
userBookArray = np.hstack(
    ([[userEncoder]] * len(notReadedBooks), notReadedBooks)
)

ratings = model.predict(userBookArray).flatten()

topRatingsIndices   = ratings.argsort()[-10:][::-1]
recommendedBookIds = [
    book_encoded_to_book.get(notReadedBooks[x][0]) for x in topRatingsIndices
]

print('Showing recommendations for users: {}'.format(userId))
print('=====' * 8)
print('Book with high ratings from user')
print('-----' * 8)

topBookUser = (
    readedBooks.sort_values(
        by = 'book_rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)

bookDfRows = datasetBook[datasetBook['ISBN'].isin(topBookUser)]
for row in bookDfRows.itertuples():
    print(row.book_title, ':', row.book_author)

print('=====' * 8)
print('Top 10 Books Recommendation')
print('-----' * 8)

recommended_book = datasetBook[datasetBook['ISBN'].isin(recommendedBookIds)]
for row in recommended_book.itertuples():
    print(row.book_title, ':', row.book_author)

